We thank the reviewers for their time and thorough reviews.

EVALUATION

Given the reviews, we have failed to convey the *purpose* of our evaluation to the reader.

We replace our vague Contribution #1 (L197) with:

 "We present a new methodology and *workflow* for the programmer to
  type check their untyped, plain-data-driven program."

Our evaluation shows that our *workflow* is highly effective compared to related
works (discussed below). Here is the workflow:

1. Run our tool, generating initial annotations
2. Type check the program with Typed Clojure (TC)
   (incorrectly annotated TC programs run as normal Clojure programs)
3. Respond to static typing errors by scanning the type annotations and changing them
4. Go to 2. until ported.

Figure 13 demonstrates that this workflow works for 5 real-world programs,
and quantifies the exact changes needed.
As reviews noted, we failed to enumerate the changes for dj&mo (fixed in final version),
but we *did* succesfully port them (see MD column).

For the type checker mini.occ, we infer 3 *mutually-recursive types* (representations for
types, propositions, and expressions) named T, P, and E, with 14 total cases (5, 5, 4, respectively).
Each uses different dispatch keys: :T, :P, and :E.

We also infer
  (defalias NameTypeMap '{:name Sym, :type T})
for function parameters types.

Manual amendment using the *workflow* (MD=46/24) consisted of:
- add: 2 cases to E, 3 to T
- amend: one overprecise T case 
         - unit tests only used 1-argument function types in T,
           so change singleton collection '[NameTypeMap] => (Vec NameTypeMap)
- change: 5 recognizably overprecise function annotations (eg. (Vec Nothing) => (Vec T))
- casts: inserted 8 casts to appease TC
- local: annotate unexercised local function with NameTypeMap, fix 1 overprecise local annotation
- other: deleted trivial type annotations for unit tests embedded in file,
         disable type checking unit tests

This shows how the workflow can constructively guide a user through the entire
process of porting an untyped program using plain-data.




EVALUTION QUESTIONS
- Looks like it's poor because you generate unsound types
  A: impossible to make such a tool
     Limitation of dynamic type inference:
      1. incomplete tests
      2. dynamic analysis difficulties (polymorphism, variable arity)
         - no explicit runtime information

     If we don't have enough data:
     1. Reject the program
      - give me more tests, THEN I'll give you answers
     2. Give a partial answer
      - targetting at people who have CLJ code and almost certainly a bunch of tests
        and would like to add annotations

     We show that even though you don't get the full answer, the user knows what
     to do next 
     (workflow:)
     - run the type checker
     - manually inspect readable types (or amend)
     - iterate

     The goal of our evaluation is to show that our tools
     fits into this workflow.

     cljs.compiler - not attempting to port, large amount of work even with our tool.
     - why did we do this evaluation despite porting
       - did enough evaluation to show promising results

     What's the point of the evaluation?
     - evaluate the WORKFLOW
       1. the workflow works out
       2. our techniques are needed for real Clojure programs
          - TypeWiz is not enough
          - we need the features that other systems don't support

     - why is it not a problem that you don't have immediately checkable/sound results
       - not plausible with the tool we're working with 
       - we want to show that this tool is helpful in practice
     - what does unported results show? 
       - mini.occ+json actually ported
       - cljs.compiler shows promise
         - showing the tool produces valuable output for large-scale, complex Clojure programs
         - discovers the key types in ClojureScript
           - need to believe our qualitative evaluation that they are the key types
             - they are, unsurprising for a compiler

/EVALUTION QUESTIONS

RELATED QUESTIONS
- not related to
  - soft-typing/quasi-static typing
    - insert runtime checks
  - no dynamic analyses in POPL2011, ESOP2012
  - Furr SAC 2009
    - no dynamic analysis

/RELATED QUESTIONS

TECHNICAL QUESTIONS
- we will fix issues
/TECHNICAL QUESTIONS


>Key novelties?

We tackle the problem of inferring annotations for untyped programs that do not explicitly
distinguish between homogeneous and heterogeneous data structures.
For example, a class/struct declaration is an explicit declaration of a heterogeneous concept,
along with a name, fields, and relationships.
Our solution is immune to the loss of such explicit concepts.

No other auto-annotation system (to our knowledge) attempts to combine program-wide information
to synthesize recursive type aliases that can be quickly manipulated by programmers.
We present a methodology, formal model, and tool that encapsulates our approach.

L151-152 demonstrates that no special effort is needed to discover recursive types for class-based programs.
We introduce the class-based example (L138-143) to be fair to TypeWiz, because a direct comparison
highlights TypeWiz's "[alarming] ... linear growth" (L150) for plain data.

Here is the (pprinted) linearly-growing left-leaning TypeWiz annotation we mention L147-150 (too large to fit in the paper):

; left leaning
function nodes(t: { left: { left: { op: string, val: number },
                            op: string, right: { op: string, val: number } },
                    op: string, right: { op: string, val: number } }
                 |{ left: { op: string, val: number },
                    op: string,
                    right: { op: string, val: number } }
                 |{ op: string, val: number })

; balanced
function nodes(t: { left: { left: { op: string, val: number },
                            op: string, right: { op: string, val: number } },
                    op: string,
                    right: { left: { op: string, val: number },
                             op: string, right: { op: string, val: number } }}
                 |{ left: { op: string, val: number },
                    op: string,
                    right: { op: string, val: number } }
                 |{ op: string, val: number }) {


In our experience, this is the state of prior work in this area---and adding 
Figure 3 is a stunning improvement on recognizability and reusability to the above---we hope
readers will deduce this just by comparing Fig 2 and 3.





>Non-contribution #1?

We replace L197:
  "We outline a new methodology for automatically annotating untyped programs
   that primarily use plain data (Section 2)."



>Typed Clojure (TC)?

The literature [5] reports that TC's lack of library annotations hinders its usability
for real programmers. The Clojure 2018 survey's asks:

  "What has been most frustrating or has prevented you from using Clojure more than you do now?".

11.7% of 1,851 chose:

  "No static typing".

This suggests TC's usability issues are preventing adoption of both itself and Clojure,
which motivates us to tailor our tool to improve TC to fill a need of real programmers.




>Runtime technique?

Our "simple dynamic analysis gives a nice baseline solution" (B)
that may be combined with other static, or statistical approaches.




>Naming unexplained/unevaluated?

Naming is a simple local analysis hinted in L242-252.
So, its effectiveness entirely depends on 'squashing' to bring together related names.
We evaluate, instead, the latter's ability to merge related concepts,
which makes creating "recognizable names" (L232) easier.

We assume that a sufficiently squashed type contains enough local information (e.g., key names)
to assign recognizable names.
L249 discusses a case where this hypothesis fails.



>Writing good tests easier than annotating code?

Clojure's "culture of unit testing" [L22] allows us to skip this question,
and take advantage of existing tests to assist porting untyped code.




>Experiment 3: No soundness guarantee? Generative testing caveat?

We amend L1225:
  "Some >generated< function specs..."
because our tool *generates* the function spec.

The caveat: spec's generative testing semantics for HOF weakens our "passes unit tests" result
over standard HOF contract checking (wrapping), because it is more permissive.




>Timing/memory information

We will include these for benchmarks (L311 is indicative of overhead).






>Non-recursive, ad-hoc data structures?

groupSimilarReq (Figure 10) merges non-recursive maps with similar required keys,
and our tool assigns a type alias name based on its keys.
This way, types are not copied throughout the program.
The downside is that types might be too broad (for example,
the same entry might be optional in one function, but required in another)
but TC will throw a type error when it is checked in that case.

See ColumnLineContextMap in Figure 5 for a real example.

We don't perform this grouping for homogeneous data-structures, but
their members can be named (eg. L1040, EnergySectorMap is named, but not
Vec). This matches our intuition of how helper functions are written in
Clojure (usually processes a single item, and then combined with map/filter).




>Handling excessive merging in subtle cases?

Say we have two types T{1,2}, and functions f{1,2} process T1 and T{1,2}, respectively.

If T_n share the same dispatch key, we will likely infer:

(defalias T (U T1 T2))
(ann f1 [T :-> T])
(ann f2 [T :-> T])

But we actually want:

(defalias T  T1)
(defalias T' (U T T2))
(ann f1 [T :-> T])
(ann f2 [T' :-> T'])

The current algorithm encourages the following manual fix:

First, the programmer finds T and uses it to create T'.
Since the tool chooses meaningful names (L232) and assumes
some familiarity with what the data represents (L239), it helps quickly
identify the underspecified AST type.

Then, a manual search for T reveals the places where T' should be used (like f2).
If it is unclear which T{,'} should be used, the type system could be used
to experiment whether adding/removing cases would throw warnings or type errors.
(TC allows a program with incorrect annotations to be run normally)

If hypotheses L345-350 do not hold, the algorithm can be tuned to
squash per-function (or further granuality) by changing the driver (L353)
to isolate runtime results after "collect", and running "infer" multiple times
on the smaller groups.

Alternatively, removing the squashGlobal pass would have similar results, which
prevents recursive types from merging between f{1,2} (our tool supports this).




>Incomplete/confusing Fig 13. Was `mini.occ`/`data.json` ported?

We will make this clear, but the MD column (Fig 13) shows we *did* port them.

The final paper will fill-in Fig 13 for mini.occ and data.json.

Only cljs.compiler was not ported due to the large number of local annotations
needed, but is a crucial stress test of 'squashing' (L993-L1000, 14/36 recursive Op
cases inferred using only 39 assertions).



>Successfully port programs with recursive types?

For mini.occ, we infer 3 mutually recursive data structures (representations for
types, propositions, and expressions) named T, P, and E, with 14 total cases (5, 5, 4, respectively).
Each uses different dispatch keys: :T, :P, and :E.

Manual amendment (MD=46/24, Fig 13):
- add: 2 cases to E, 3 to T
- amend: one overprecise T case (no. function arguments can be non-1)
- change: 5 recognizably overprecise function annotations (eg. (Vec Nothing) => (Vec T))
- casts: inserted 8 casts for TC
- other: deleted trivial type annotations for unit tests embedded in file

Adding various local annotations to appease the type checker.




>Misleading title: not 'collapsing' much work.

Our evaluation supports our title after a few small fixes.
The Git line diff (MD, Fig 13) is insufficient, which we try to supplement
with all the columns to right of it.
Our experience shows the cognitive overhead saved is significant.

For example, for mini.occ, the programmer was saved from conceptualizing,
naming, or even specifying-the-relationship-between 3 mutually recursive
heterogeneous data structures with over a dozen cases. 
Their job, instead, becomes identifying for "recognizable names" for minor fixes and 
following the instructions of TC.

Furthermore, in our experience (like above), changes are often obvious by sight (eg. overprecise argument, the "O" metric)
or indicated by the type checker (eg. casts, the "C" metric).



>Emit more interesting contracts?

Yes, and this information could be collected by our runtime analysis, and could be
added as the need/demand arises. Memory during collection
and processing time for extra samples are the main concerns.



>Why not start the evaluation with a corpus of type-annotated code, which would allow to remove and infer again the type annotations? 

TC users are discouraged by annotating both their own code and libraries [5]. We present
the messy experience of automatically annotating such real-world cases because:

- Using real code allows us to fairly test our hypotheses about Clojure idioms
- We believe pre-annotated would be misleadingly easier to port than always-untyped because
  TC enforces stricter discipline. We chose the more challenging representative benchmarks of real, always-untyped code.

Our results seem weaker (merely messier, in our opinion) from avoiding misleading results.




>Awkward/spotty technical presentation

We will:
- introduce 'paths' with an example early in Section 3.
- include delarative specifications for introduced functions.
- cite Fig 9-10
- clarify types+expressions we omit from formalism but appear in other discussions



>Lazy+eager mixing? (L906)

In practice, we use lazy tracking for maps, but forcing a small depth/width
of nested tracking to be realized eagerly helps gather enough information for effecive 'squashing'
without significant runtime cost.




>Evaluate tool on unsuspecting users, or comparing migration time

Our approach eases the cognitive overhead that is unique to annotating plain-data-driven
programs. It does not claim to 
