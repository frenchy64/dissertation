\documentclass[11pt]{iuthesis}
%\documentclass[11pt,final]{iuthesis}

% this is the dissertation, not the paper
\newcommand{\DISSERTATION}{}
\newcommand{\either}[2]{#1}

%\usepackage[margin=1in]{geometry}
\usepackage{savesym}
\savesymbol{r}
\savesymbol{AA}
\usepackage{esop-common}
\usepackage{infer-common}
\usepackage{symb-common}
\usepackage{quals-common}

% related to typesetting theorems, moved here for compatibility with acmart.cls
%\newcommand\@dotsep{12}

\usepackage[doublespacing]{setspace}

%\usepackage{hyperref}

\newcommand{\thesisauthor}[0]{Ambrose Bonnaire-Sergeant}
\newcommand{\thesistitle}[0]{Typed Clojure in Theory and Practice}
%\newcommand{\thesiskeywords}[0]{Kwd1, Kwd2, Kwd3}
\newcommand{\thesismonth}[0]{TODO}
\newcommand{\thesisyear}[0]{TODO}
\newcommand{\thesisdate}[0]{\today}

%% Setup for hyperref.
%\hypersetup{
%  pdftitle={\thesistitle{}},
%  pdfauthor={\thesisauthor{}},
%  colorlinks=true,
%  linkcolor=black,
%  citecolor=black,
%  urlcolor=black,
%}

%\usepackage[T1]{fontenc}

\advisor{Sam Tobin-Hochstadt}
\secondreader{Chung-chieh Shan}
\thirdreader{Ryan R. Newton}
\fourthreader{Lawrence S. Moss}
\departmentname{School}
\department{Informatics, Computing, and Engineering}
\copyrightyear{2019}
\submitdate{TODO}
\acceptdate{TODO}

% For use with iuthesis-alt.cls.
\parskip=6pt
\parindent=0pt
\normalparindent=0pt

\usepackage[top=1in,bottom=1.25in,left=1in,right=1in]{geometry}
\renewcommand{\thepart}{\Roman{part}}
\renewcommand{\thechapter}{\arabic{chapter}}

% The annoying section-only section numbering is inherited from
% amsbook, on which iuthesis-alt is based.  This is to bring back
% chapter numbers in the section headings.
\renewcommand{\thesection}{\thechapter.\arabic{section}}
\renewcommand{\thesubsection}{\thechapter.\arabic{section}.\arabic{subsection}}

% Also, put chapter numbers in figure and table numbering.
\renewcommand{\thefigure}{\arabic{chapter}.\arabic{figure}}
\renewcommand{\thetable}{\arabic{chapter}.\arabic{figure}}

% fix badly formatted toc
% https://tex.stackexchange.com/questions/22983/list-of-figures-and-list-of-tables-overlaps-figure-table-indices-with-proceeding
\makeatletter
\renewcommand*\l@figure{\@dottedtocline{1}{1.5em}{3em}}% 3em instead of 2.3em
\let\l@table\l@figure
\makeatother


\usepackage[backend=bibtex]{biblatex}
\addbibresource{bibliography.bib}

%\addtolength{\textwidth}{.5in}
%\addtolength{\textheight}{.5in}
%\setlength{\topmargin}{.25in}
%\usepackage{fontspec}
%\usepackage{hanging}
%\usepackage{xltxtra}
%\setlength{\oddsidemargin}{.75in}
%\setlength{\evensidemargin}{.25in}



\usepackage{thmtools}
%\declaretheorem[numberwithin=chapter]{example}
%\declaretheorem[numberwithin=chapter]{theorem}
%\declaretheorem[numberwithin=chapter]{lemma}
%\declaretheorem[numberwithin=chapter]{corollary}
%\declaretheorem[numberwithin=chapter]{definition}

\begin{document}

\frontmatter %turns off chapter numbering and uses roman numerals for page numbers
\title{\thesistitle{}}
\author{\thesisauthor{}}

%\begin{acknowledgements}
%\end{acknowledgements}

%\begin{dedication}
%\end{dedication}


\maketitle
\signaturepage
\copyrightpage
%\makeack
%\makededication
%\makeabstract

\input{abstract}

\singlespacing
\tableofcontents

%\listoffigures

%\listoftheorems
\doublespacing

\newpage

%turns on chapter numbering, resets page numbering and uses arabic numerals for page numbers;
\mainmatter

%\input{thesis-statement}

\chapter{Introduction}

\section{My Thesis}

\emph{Typed Clojure is a sound and practical optional type system for Clojure.}

\section{Structure of this Dissertation}

This document progresses in several parts that support my thesis statement, presented in chronological order.

Part~\ref{part:types} motivates and presents the design of Typed Clojure.
It addresses both parts of my thesis statement.

\begin{itemize}
  \item \emph{Typed Clojure is sound} I formalize Typed Clojure, including
    its characteristic features like hash-maps, multimethods, and Java interoperability,
    and prove the model type sound.
  \item \emph{Typed Clojure is practical} 
      I present an empirical study of real-world Typed Clojure usage
        in over 19,000 lines of code, showing its features correspond to actual usage patterns.
\end{itemize}

The results and industry feedback of this work inspired three distinct research directions
to help improve the experience of using Typed Clojure.

\begin{itemize}
  \item
\partref{part:autoann} presents a solution to lower the annotation burden in real-world Typed Clojure programs.
I formalize and implement a tool to automatically annotate types for top-level
user and library definitions, and empirically study the manual changes needed for the generated annotations
to pass type checking.
  \item
\partref{part:implementations} describes the design and implementation of a 
new code analyzer for Clojure, in service of enabling user-provided type rules for Clojure macros
    to help make type checking complex macro usages more robust.
\item \partref{part:symbolic-closures} motivates and describes \emph{symbolic closure types},
      a technique that enhances type checking with symbolic execution, that helps check some
      common Clojure idioms via a compatible extension of Typed Clojure's original design.
\end{itemize}

Finally, \partref{part:related-future-work} presents the related work and future directions for each part.

\part{Practical Optional Types for Clojure}
\label{part:types}

\input{esop-abstract}
\input{esop-intro}
\input{esop-overview}
\input{esop-formal-model}
\input{esop-metatheory}
\input{esop-experience}
\input{esop-conclusion}

\part{Automatic Annotations for Typed Clojure}
\label{part:autoann}

\chapter{Abstract}
\input{infer-abstract}

\input{infer-intro}
\input{infer-overview}
%\input{infer-algorithm} % old stuff
\input{infer-formalism}
\input{infer-evaluation}
\input{infer-extensions}
% infer-comparison == performance analysis vs Daikon
% this isn't in the PLDI submission so might not really fit yet
%\input{infer-comparison}
\input{infer-conclusion}

% Quals
%\part{Investigation of clojure.spec}
%\label{part:spec}
%
%\input{spec-intro}
%\input{spec-study}
%\input{spec-model}

\part{Typed Clojure Implementations}
\label{part:implementations}
\input{analyzer}

\part{%Local Type Argument Synthesis with Symbolic Closures
Symbolic Closures}
\label{part:symbolic-closures}

\input{symbolic}

\part{Related and Future Work}
\label{part:related-future-work}

\input{esop-related-work}

\input{infer-related-work}

%% NOTE: Haven't pursued the following work yet

%\paragraph{How dynamic languages are used}
%Several languages have seen similar investigations
%into their idioms as I am proposing for Clojure.
%
%A popular motivation is to discover which type system features to support
%when retrofitting a type system.
%% FIXME the is \AAkerblom but there's an error.. also in the bibliography
%Akerblom et. al~\cite{Akerblom:2014:TDF:2597073.2597103} trace dynamic features in Python programs
%via instrumentation. They measured the prevalence of dynamic features in startup versus
%user code, and recorded usage frequencies for a set of dynamic features.
%They concluded dynamism is prevalent in Python, and thus should be supported
%in a retrofitted type system for Python.
%A study along similar lines is also applicable to Clojure, in particular analysing Typed
%Clojure's support for Clojure's dynamic features.
%
%Calla{\'u} et al. \cite{Callau2013} also conducted a large-scale study of
%dynamic Smalltalk idioms to inform future language extensions tooling support.
%Notably, they further perform a qualitative analysis aiming to identify
%the reasons why Smalltalk use these features in the first place, and
%whether they can be replaced with more predictable features. They also 
%measure which kinds of projects (e.g., testing frameworks, user-level libraries, or core system libraries) 
%use particular features more frequently.
%Due to the their prevalence in the open-source Clojure ecosystem,
%Typed Clojure has mainly been tested on user-level libraries.
%We could predict Typed Clojure's applicability to other kinds of projects
%by gathering similar data on how frequently different types of Clojure libraries use
%Clojure's various features.
%
%Andreasen et. al~\cite{Andreasen2016TraceTA} developed
%\emph{trace typing} to explore the design space of JavaScript type systems. 
%Using runtime observations, they studied which control flow techniques
%are used most often in JavaScript programs, and thus, which should
%be supported by an effective type system for JavaScript.
%Typed Clojure implements occurrence typing to reason about control
%flow in Clojure which seems to work well in practice, but a similar
%quantitative analysis could reveal further insights.

%Runtime analysis \cite{Mastrangelo:2015:UYO:2814270.2814313}

%\cite{Mastrangelo:2015:UYO:2814270.2814313} 

\chapter{Related Work to Extensible Type Systems%and Interleaving Type Checking with Expansion
}

Turnstile~\cite{Chang2017TSM} type checks a program during expansion
by repurposing the Racket macro system.
Instead of the more standard approach of providing separate rules to check a macro, Turnstile
typing rules specify both the expansion and checking semantics, and so ensuring the
two are compatibile becomes automatic.
On the other hand, Typed Clojure does not have the goal of allowing users to override
how language primitives type check. Instead, our goal is to provide
a simple interface to write type rules for library functions and macros
in a style that hides the necessary bookkeeping surrounding occurrence
typing and scope management.

SugarJ~\cite{Erdweg2011SJ}
adds syntactic language extensibility to languages like Java, such as pair
syntax, embedded XML, and closures.
Desugarings are expressed as rewrite rules to plain Java.
Similarly, work on \emph{type-specific languages}~\cite{omar2014safely}
adds extensible systems for the definitions of specialized syntax literals
to existing languages.
The \emph{type} of an expression determines how it is parsed and elaborated.

% this paper has a great related works section that differentiates
% the strategies of several typed metaprogramming techniques
SoundX~\cite{Lorenzen2016STS} presents a solution to a common
dilemma in typed metaprogramming: whether to desugar before
type checking, or vice-versa.
The authors present a system where a form is type checked before 
being desugared, with a guarantee that only well-typed code is generated.
Programmers specify desugarings with a combination of typing and rewriting rules, 
which are then connected to form a valid type derivation
in a process called \emph{forwarding}.
We will explore whether we can get the same effect in Typed Clojure
without requiring the user to understand typing rules.
%For example, Scala macros~\cite{Burmako2013SML} interleave type checking and
%desugaring

Ziggurat~\cite{Fisher06staticanalysis} allows programmers to define
the static and dynamic semantics of macros separately. To demonstrate its
broad applicability, they choose Scheme-like macros that generate assembly code
for the dynamic semantics.
They advocate building towers of static analyses, so
macros can be statically checked in terms the static semantics of other macros, instead
of just their assembly code expansions which would otherwise be too difficult to check.
This idea resembles our prototypes in defining custom typing rules for functions and macros in Typed Clojure,
where the dynamic semantics are defined by runtime Clojure constructs (\texttt{defn}
and \texttt{defmacro}), and towers of static semantics are progressively specified in terms of the static
analysis of other Clojure forms.

Type Tailoring~\cite{greenmanttailoring} is an approach to provide more information
to a host type system than it might be capable of by itself.
In particular, the authors use the host platform's metaprogramming functionality
to refine the types of calls based on the program syntax alone, as well as improve
error messages by incorporating surface syntax. Their experiments are based in Typed Racket, that fully expands
syntax before checking it. Since Typed Clojure recently changed to interleave macroexpansion
and type checking, we could extend this technique to also refine calls based on the
types of their arguments (like SoundX).

Other work is relevant to our investigations of improving the user experience
of Typed Clojure. SweetT~\cite{pombrio2018inferring} automatically infers type rules
for syntactic sugar. Helium~\cite{Heeren2003STI} provides hooks into the type inference
process for domain-specific type error messages.

\chapter{Related Work to Symbolic Closures% and Combining Type Checking with Symbolic Analysis
}

\paragraph{Local Type Inference}
Symbolic closures were originally designed as an extension of Local Type Inference~\cite{PierceLTI}.
Our presentation omitted their bidirectional checking (we did not propagate type information down the syntax
tree using synthesis/checking rules)
and so was not a superset of Local Type Inference---in
particular, it does not take advantage of the relaxed optimality conditions when inferring type
arguments in checking mode.
However, adding back bidirectional checking should be possible by starting with the rules of Local
Type Inference, adding a \emph{synthesis} rule for functions that introduces a symbolic
closure, application and subtyping rules for symbolic closures, and some side conditions to restrict 
how a symbolic closure may be reasoned about (like our ``must not contain symbolic closures''
conditions scattered in various rules).
This way, a symbolic closure should only be introduced where Local Type Inference fails---when
a type of a function must be synthesized---and so seems more likely to be a superset of 
Local Type Inference.

Colored Local Type Inference~\cite{coloredlti01} extends Local Type Inference with partial
information propagation. Their type inference algorithm does not use explicit synthesis/checking
rules, instead passing ``prototypes'' \emph{P}
down the syntax tree that containing partial expected type information used for type checking.
A prototype is a type \emph{T} extended with the wildcard ``?'', denoting unknown information,
and the specific shape of a prototype denotes which type rule to use.
The rule inferring unannotated functions \ltiufun{\ltivar{}}{\ltiE{}} requires a prototype \ltiPolyFn{T}{}{P}, where
\emph{T} is the fully known expected type for {\ltivar{}}.
A symbolic closure could be introduced when checking an unannotated function with prototype
\ltiPolyFn{P}{}{P'} or ``?'' (the equivalent of Local Type Inference's ``synthesis'' rules).
In the more complicated case of \ltiPolyFn{P}{}{P'}, a symbolic reduction of 
\ltiufun{\ltivar{}}{\ltiE{}} is required to ensure it at least conforms its the prototype.
For example, inferring the type of \ltiapp{\text{map}}{\ltiufun{\ltivar{}}{\ltiE{}},[1,2,3]} with
Colored Local Type Inference (where \text{map} has type ``\ltiPolyFn{\ltiFn{\text{a}}{\text{b}},\text{List[a]}}{\text{a,b}}{\text{List[b]}}'')
checks \ltiufun{\ltivar{}}{\ltiE{}} with prototype \ltiPolyFn{\text{?}}{}{\text{?}}.
We can be optimistic and check the function 
at the largest (most specific) subtype of 
\ltiPolyFn{\ltiBot}{}{\ltiTop}
that matches \ltiPolyFn{\text{?}}{}{\text{?}}, which is \ltiPolyFn{\ltiBot}{}{\ltiTop}.
This ensures that the function at least conforms to the most optimistic interpretation of its
prototype, and then by returning a symbolic closure type instead of \ltiPolyFn{\ltiBot}{}{\ltiTop}
allows us to check more specific requirements later.
Of course, to fully check this example, it requires that we specify how type argument synthesis works with
symbolic closures, but it at least illustrates how symbolic closures relate to the rest of the system.

Spine-local type inference~\cite{DBLP:journals/corr/abs-1805-10383}
explores Local Type Inference in the context of System F (without subtyping).
They present a greedy type argument synthesis algorithm
which more aggressively propagates type information
to an application's arguments.
To check arguments, type variable instantiations are guessed
based on the expected type of the application.
For example, when checking \ltiapp{\text{id}}{\ltiufun{\ltivar{}}{\ltiE{}}}
with expected type \ltiFn{\ltiT{}}{\ltiS{}},
where \text{id} has type \ltiPolyFn{\ltitvar{}}{\ltitvar{}}{\ltitvar{}},
\ltitvar{} would be guessed to have type \ltiFn{\ltiT{}}{\ltiS{}}
and then {\ltiufun{\ltivar{}}{\ltiE{}}} would be checked at that type.
This would fail if the application was in synthesis mode.
In this specific example, symbolic closures would allow the checking
of \ltiufun{\ltivar{}}{\ltiE{}} to be delayed to when more type information
is available, in either checking or synthesis modes.
Unfortunately, it does not seem that their algorithm can check
\ltiapp{\text{map}}{\ltiufun{\ltivar{}}{\ltiE{}},[1,2,3]}
even in checking mode, and so does not seem to assist us in solving similar
problems with symbolic closures.
This case does not check because only the type of \ltiE{}
would be apparent from an expected type, not the type of \ltivar{}.

% Spine-local type inference
% - Judgement \vdash^P digs down an application to find the head
%   - happens naturally with symbolic closures
% - they use metavariables to solve direct applications
%   - can they check things like (let [x (fn [y] (inc y))] (x 1)) ?
% - they have polymorphism but not subtyping (plain System F)
%   - they speculate about extending to Fsub in related works
%   - they mention Hosoya & Pierce's "challenges" to fix hard-to-synthesize terms
% - they type check arguments left-to-right in a polymorphic application
%   - can't tell if that's different from inferring the data flow from a polytype 
%     and then checking in that order
% - their sense of "locality" is less ambitious than symbolic closures
%   - see "Type Inference Failures" section
% - good related works section for "Impredicative Polymorphism"

\paragraph{Mixing Symbolic Execution and Type Checking}
Mix~\cite{Khoo2010MTC} allows an interplay of symbolic execution~\cite{King1976SEP} with type checking
by providing syntactic regions,
with terms
\MixTregion{\ltiE{}} signaling to use type checking for {\ltiE{}},
and
\MixSregion{e} for symbolic execution.
In Mix, for example, the term
%
\[
\MixSregion{\ltilet{\text{id}}{\ltiufun{\ltivar{}}{\ltivar{}}}
                  {\MixTregion{  ...\ \MixSregion{\ltiapp{\text{id}}{3}}
                               \ ...\ \MixSregion{\ltiapp{\text{id}}{3.0}}
                               \ ... }}}
\]
%
symbolically executes \MixSregion{\ltiapp{\text{id}}{3}}
and
\MixSregion{\ltiapp{\text{id}}{3.0}}, propagating result types
\text{Int} and \text{Real} back to the typed regions, respectively.
Comparatively, symbolic closures integrates only a small amount of
symbolic execution with a type system, but in such a way that delayed symbolic computations
may pass between typed regions.
Since Mix cleanly separates symbolic execution and type checking, it is difficult
to compare the two approaches.
In rough terms, symbolic closures use typed regions by default and automatically adds symbolic regions
around unannotated functions.
%
\[
\MixTregion{\ltilet{\text{id}}{\MixSregion{\ltiufun{\ltivar{}}{\ltivar{}}}}
                   {  ...\ \ltiapp{\text{id}}{3}
                    \ ...\ \ltiapp{\text{id}}{3.0}
                    \ ... }}
\]
%
Typing rules are then added to introduce a symbolic closure type
and also to apply them, which involves checking the delayed body in a typed region.
%
\begin{mathpar}
\infer[]
  {}
  { \ltitjudgementNoElab{\ltiEnv{}}{\MixSregion{\ltiufun{\ltivar{}}{\ltiE{}}}}
                        {\ltiClosure{\ltiEnv{}}{\ltiufun{\ltivar{}}{\ltiE{}}}}
  }

\infer[]
  { \ltitjudgementNoElab{\ltiEnv{}}{\MixTregion{\ltiF{}}}
                        {\ltiClosure{\ltiEnvp{}}{\ltiufun{\ltivar{}}{\ltiEp{}}}}
    \\\\
    \ltitjudgementNoElab{\ltiEnv{}}{\MixTregion{\ltiE{}}}{\ltiS{}}
    \\
    \ltitjudgementNoElab{\ltiEnvConcat{\ltiEnvp{}}{\hastype{\ltivar{}}{\ltiS{}}}}
                        {\MixTregion{\ltiEp{}}}
                        {\ltiT{}}
  }
  { \ltitjudgementNoElab{\ltiEnv{}}{\MixTregion{\ltiapp{\ltiF{}}{\ltiE{}}}}{\ltiT{}}
  }
\end{mathpar}

When forced to delineate type checking from symbolic execution like this, it interesting to ask to what degree symbolic
closures even uses symbolic execution.
Our view is that (at least) symbolic closures symbolically execute the runtime-closure introduction rule.
%
\begin{mathpar}
\infer[]
  {}
  {
  \opsem {\openv{}}
         {\ltiufun{\x{}}{\e{}}}
         {\closurenosuffix{\openv{}}{\ltiufun{\x{}}{\e{}}}}
         }
\end{mathpar}
%
The symbolic closure type
{\ltiClosure{\ltiEnv{}}{\ltiufun{\ltivar{}}{\ltiE{}}}}
is then the symbolic value of the runtime closure
{\closurenosuffix{\openv{}}{\ltiufun{\x{}}{\e{}}}},
related by the following typing rule.
%
\begin{mathpar}
\infer []
{ 
  \overrightarrowcaption{
  \ltitjudgementNoElab{}{\ltiEnvLookup{\openv{}}{y}}{\ltiEnvLookup{\ltiEnv{}}{y}}
  }
  ^{y \in dom(\openv{})}
              }
{ \ltitjudgementNoElab {\ltiEnvp{}}
                       {\closurenosuffix
                        {\openv{}}
                        {\ltiufun{\ltivar{}}{\ltiE{}}}}
                       {\ltiClosure{\ltiEnv{}}{\ltiufun{\ltivar{}}{\ltiE{}}}}
          }
\end{mathpar}
%
As evidenced by the lack of symbolic regions in the above application rule,
a ``symbolic reduction'' of a symbolic closure is not particularly related
to symbolic execution---it merely kicks off some delayed type checking.
However, \ltiEnv{}, \ltiS{}, and \ltiT{} in that rule may contain symbolic closure
types, so symbolic values are being used to reason about the program.

%\begin{mathpar}
%\infer[]
%{ \opsem {\openv{}}
%         {\e{f}}
%         {\closurenosuffix {\openv{c}} {\abs {\x{}} {\t{}} {\e{b}}}}
%         \\
%  \opsem {\openv{}}
%         {\e{a}}
%         {\v{a}}
%         \\
%  \opsem {\extendopenv {\openv{c}} {\x{}} {\v{a}}}
%         {\e{b}}
%         {\v{}}
%       }
%{ \opsem {\openv{}}
%         {\appexp {\e{f}} {\e{a}}}
%         {\v{}}
%       }
%\end{mathpar}

%Symbolic closures type check an anonymous function if annotated, otherwise it is treated symbolically.
%As the authors envision, this is akin to automatically inserting
%the mode of a code region based on its context, with a Mix-like language
%becoming the intermediate language.

Mix also uses symbolic execution to enhance simple type systems with flow-sensitivity.
For example, the following Mix program uses symbolic execution 
to flow-sensitively reason about \text{int?}, a predicate that returns true only for integer values.
%
\[
\MixSregion{\ltilet{\text{f}}{\ltiufun{\ltivar{}}{(\ltiif{\ltiapp{\text{int?}}{\ltivar{}}}{\ltivar{}}{\textsf{nil}})}}
                  {\MixTregion{  ...\ \MixSregion{\ltiapp{\text{f}}{3}}
                               \ ...\ \MixSregion{\ltiapp{\text{f}}{3.0}}
                               \ ... }}}
\]
%
The symbolic regions determine
\MixSregion{\ltiapp{\text{f}}{3}} has type \text{Int} and
\MixSregion{\ltiapp{\text{f}}{3.0}} type \text{Nil} via symbolic execution.
Symbolic closures are instead designed to be compatible with flow-sensitive type systems like occurrence typing~\cite{TF10}.
Here is the analogous program using symbolic closures.
%
\[
\MixTregion{\ltilet{\text{f}}{\MixSregion{\ltiufun{\ltivar{}}{(\ltiif{\ltiapp{\text{int?}}{\ltivar{}}}{\ltivar{}}{\textsf{nil}})}}}
                  {  ...\ {\ltiapp{\text{f}}{3}}
                               \ ...\ {\ltiapp{\text{f}}{3.0}}
                               \ ... }}
\]
%
Now, let us assume occurrence typing is also used to check this program, and
that \text{int?} is typed as a predicate for integers.
The call to
{\ltiapp{\text{f}}{3}}
triggers the symbolic reduction
%
\[
\ltitjudgementNoElab{\hastype{\ltivar{}}{\text{Int}}}
                    {(\ltiif{\ltiapp{\text{int?}}{\ltivar{}}}{\ltivar{}}{\textsf{nil}})}
                    {\text{Int}}
\]
%
which has type \text{Int}, because the else-branch is unreachable, and 
similarly {\ltiapp{\text{f}}{3.0}} triggers
%
\[
\ltitjudgementNoElab{\hastype{\ltivar{}}{\text{Int}}}
                    {(\ltiif{\ltiapp{\text{int?}}{\ltivar{}}}{\ltivar{}}{\textsf{nil}})}
                    {\text{Nil}}
\]
%
which has type \text{Nil}, because the else-branch is unreachable.

% >> talk about occurrence tpying.

% Let arguments go first

% Dunfield works I need to compare to
% - Greedy Bidirectional Polymorphism
% - Sound and complete bidirectional typechecking for higher-rank polymorphism with existentials and indexed types
% - Complete and Easy Bidirectional Typechecking for Higher-Rank Polymorphism

% Other works with undecidable type checking
% - Hybrid type checking - Knowles, Flanagan

\paragraph{Intersection Type Checking}
In hindsight, the idea behind symbolic closures resembles intersection type checking,
where the same code may be checked at multiple types.
Carlier and Wells give an approachable explanation of ``expansion'' in their survey paper~\cite{carlier2005expansion},
a mechanism that informs an intersection type system when it should
check the same term at different types.
This is achieved by splicing typing rules (like intersection-introduction) into existing typing
derivations derived from 
In contrast, symbolic closures delay the construction of a typing derivation 

%In Section 3.1, they provide a motivating example for expansion, constructed to be untypable 
%with simple (non-recursive) types, and show how expansion assigns it a type.


% survey
% Expansion: the Crucial Mechanism for Type Inference with Intersection Types: A Survey and Explanation1
% https://www.sciencedirect.com/science/article/pii/S1571066105050656
% - "expansion" seems similar to inferring intersection types via symbolic closures
%   - Section 6.2 talks about type inference for rank-2 intersection types
%     - advantage is that expansion never has to introduce intersections under ->
%       - do we do this with symbolic closures?
% - "Expansion is an operation on typings that simulates the effect of splicing in typing rules
%    uses at nested positions in some derivation of that typing."
% - omega expansion looks very similar to symbolic closures types (Section 4)
%   - at least in that it embeds the term in the type to track its origin 
% - Section 5.2 talks about cost of type inference == beta reduction

% (Intersection type systems)
% Principal Types and Unification for Simple Intersection Type Systems
% https://www.sciencedirect.com/science/article/pii/S0890540185711418

\paragraph{Let-polymorphism}


\paragraph{Directional Polymorphism}

% MLsub
% https://www.cl.cam.ac.uk/~sd601/thesis.pdf
% https://www.cl.cam.ac.uk/~sd601/papers/mlsub-preprint.pdf
%
% Polar type system (Jim)
% http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.123.8718&rep=rep1&type=pdf

% Pottier
% Simplifying Subtyping Constraints: A Theory
% http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.41.7032&rep=rep1&type=pdf
% A Framework for Type Inference with Subtyping%
% http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.55.2364&rep=rep1&type=pdf

% ML_F
% http://gallium.inria.fr/~remy/mlf/mlf-type-inference-long.pdf

% \paragraph{Hindley-Milner}

%TODO
%\input{hm-comparison}

%TODO
%Xie and Oliveira~\cite{xie2018let} present a type system where
%argument type information flows to the function position in applications.
%Then, defining `let` as sugar propagates enough information to avoid
%a custom rule for `let`.
%No information is propagated from functions to applications, so the benefits
%of Colored Local Type Inference are negated.

\chapter{Future work}

% Possible future work on Higher-rank types
% - see https://www.microsoft.com/en-us/research/publication/practical-type-inference-for-arbitrary-rank-types/
%   - looks like the journal version of boxy types?
%   - some notes from the paper
%     - a predicative type system only allows a polytype to be instantiated with monotypes
%     - ML_F is both impredicative and supports type inference (but costly to implement & formalize)
%       - also infers principal types
%     - higher-kinded types are orthogonal to higher-rank types, and Haskell's implementation of the former
%       happen to work well with higher-rank types (but no explanation)
%     - the concept of "syntax-directed" rules is given lots of a explanation
%     - \vdash^inst compares two _polytypes_
%     - Kfoury and Wells 1994 show that typeability of System F (with completely erased annotations) is decidable for rank 2 
%       but undecidable for rank 3>=
%     - LTI == "partial" type inference
%       - in the sense that it's not-complete (can't check all programs)
%     - nice discussion of partial type inference


\printbibliography


\newpage
\input{esop-appendix}

\end{document}
